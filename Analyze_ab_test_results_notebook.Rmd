---
title: "Analyze A/B Test Results"
output:
  html_document:
    toc: true
---

```{r}
library(boot)
library(BSDA)
library(dplyr)
library(ggplot2)
library(readr)
```

## Wrangle

### Gather and Assess

```{r}
tbl <- read_csv("ab_data.csv")
```

```{r}
head(tbl)
```

```{r}
records <- nrow(tbl)
records
```

```{r}
# Number of records with unique users
records_unique_usr <- length(unique(tbl$user_id))
records_unique_usr
```

```{r}
records_non_unique_usr <- records - records_unique_usr
records_non_unique_usr
```

There are 3894 records with non-unique user IDs.

```{r}
# Preview records with non-unique user IDs
tbl %>%
  filter(duplicated(user_id) | duplicated(user_id, fromLast = TRUE)) %>%
  arrange(user_id) %>%
  slice_head(n = 10)
```

The redundant user IDs have mismatched records! For instance, user `630052` is
part of the `treatment` group. As such, this user should have been served the
`new_page`. Yet user `630052` is recorded as having received both. Similarly,
user `630320` is recorded as being part of the `control` group _and_ the
`treatment` group. So in which group is the user? In both instances, user
`630320` is said to have landed on the `old_page`, which should have been served
to the `control` group.

```{r}
treat_old <- tbl %>%
  filter(group == "treatment" & landing_page != "new_page")

nrow(treat_old)
```

```{r}
ctrl_new <- tbl %>%
  filter(group == "control" & landing_page != "old_page")

nrow(ctrl_new)
```

```{r}
mismatch <- bind_rows(treat_old, ctrl_new)
mismatch_cnt <- nrow(mismatch)

mismatch_cnt
```

```{r}
records_non_unique_usr - mismatch_cnt
```

All but _one_ of the non-unique user IDs are due to redundant, mismatched
record pairs.

```{r}
# Return non-unique user IDs _not_ due to redundant, mismatched record pairs.
tbl %>%
  anti_join(mismatch) %>%
  filter(duplicated(user_id) | duplicated(user_id, fromLast = TRUE))
```

```{r}
# Test whether there are any missing values in the data frame as a whole.
colSums(is.na(tbl)) > 0
```

### Clean

#### Issue 1: There are 3893 redundant user IDs having mismatched records.

The `control` group should be served the `old_page`, and the `new_group` should
be served the `new_page`.

##### Define: Filter for the properly matched results, storing them in a new data frame.

##### Code

```{r}
tbl_2 <- tbl %>%
  filter(
    (group == "control" & landing_page == "old_page") |
    (group == "treatment" & landing_page == "new_page")
  )
```

##### Test

```{r}
tbl_2 %>%
  filter(
    (group == "control" & landing_page != "old_page") |
    (group == "treatment" & landing_page != "new_page")
  )
```

#### Issue 2: There is one other non-unique user ID.

Note: The two records have different timestamps. Since the variables of
interest are `group`, `landing_page`, and `conversion`, one of the records can
be dropped.

##### Define: Drop one of the records.

##### Code

```{r}
tbl_2 <- tbl_2 %>%
  distinct(user_id, .keep_all = TRUE)
```

##### Test

```{r}
tbl_2 %>%
  filter(duplicated(user_id) | duplicated(user_id, fromLast = TRUE))
```

## Analyze

### Describe: Probability

```{r}
# Overall probability of a user converting
p_pop <- mean(tbl_2$converted)
p_pop
```

```{r}
# Control group's probability of a user converting
tbl_2_ctrl <- tbl_2 %>%
  filter(group == "control")
ctrl_cnvt_prob <- mean(tbl_2_ctrl$converted)

ctrl_cnvt_prob
```

```{r}
# Treatment group's probability of a user converting
tbl_2_treat <- tbl_2 %>%
  filter(group == "treatment")
treat_cnvt_prob <- mean(tbl_2_treat$converted)

treat_cnvt_prob
```

```{r}
# Actual difference between each group's probability of a user converting
obs_diff <- treat_cnvt_prob - ctrl_cnvt_prob
obs_diff
```

<sup>Note: `control` is subtracted from `treatment` to show how exposure to the
new landing page affects the outcome of interest `conversion`: thus, a negative
number indicates a lower probability of conversion, and a positive number
indicates a higher probability.</sup>

In our data sample, we observe that the probability of conversion for the
treatment group is lower--albeit by only 0.0016. But how can we ensure that this
accurately represents the user population as a whole--and isn't just due to
variability and randomness?

### A/B Test Results

As a starting point, let us assume that the `old_page` is better ($H_0$). To
accept the `new_page` as better ($H_1$), we will require conclusive evidence
with, say, a 95% chance of being right--ie a 5% chance of being wrong ($α$ =
0.05).

$$
H_0: p_{new} - p_{old} \leq 0\\ H_1: p_{new} - p_{old} > 0
$$

Under the null hypothesis $H_0$, our aforementioned starting point, we assume
that $p_{new}$ and $p_{old}$ are equal for the purposes of the experiment.
Further, we assume that both are equal to $p_{pop}$, the overall probability of
a user converting across our population. Basically, we assume the
landing page served makes no difference.

#### Bootstrap

We can use the bootstrap technique to measure the level of uncertainty. By
repeatedly resampling observations from the data set with replacement, we
can obtain, say, 1000 estimates for our statistic ie the difference in
conversion rates between treatment and control ($\overline{x}_1 -
\overline{x}_2$). Then, we can approximate the true difference, that is, the
difference for the overall population by taking the mean of the differences,
i.e.,

$$
\overline{B} = \frac{1}{1000} \sum_{r'=1}^{1000}
\,(p_{\vphantom{l}new}^{\,*r'} - p_{old}^{\,*r'})
$$
along with its standard deviation:
$$
\mathrm{SD}_{\overline{B}} \,(p_{new} - p_{old}) =
\sqrt{\frac{1}{1000 - 1} \sum_{r'=1}^{1000}
\,(p_{\vphantom{l}new}^{\,*r'} - p_{old}^{\,*r'} - \overline{B})^2}
$$

<sup>Note: In this context, $p_{new}$ and $p_{old}$ are sample-specific means
($\overline{x}_1$ and $\overline{x}_2$). They represent the _estimated_
conversion rates for the two groups.

##### Resample

```{r}
ab_test_bootfun <- function(data, indices) {
  sample_ctrl <- data[indices, ]
  sample_treat <- data[-indices, ]
  # Probability of a user converting for the bootstrap samples
  sample_ctrl_cnvt_prob <- mean(sample_ctrl$converted)
  sample_treat_cnvt_prob <- mean(sample_treat$converted)
  # Calculate the difference between the conversion rates
  p_diffs <- sample_treat_cnvt_prob - sample_ctrl_cnvt_prob
  return(p_diffs)
}
```

```{r}
set.seed(42)
```

```{r}
reps <- boot(data = tbl_2, statistic = ab_test_bootfun, R = 1000)
```

```{r}
# Collect garbage: free unused memory
invisible(gc())
```

```{r}
# Approximate true difference
p_diffs_mean <- mean(reps$t)
p_diffs_mean
```

```{r}
# And standard deviation
p_diffs_mean_sd <- sd(reps$t)
p_diffs_mean_sd
```

```{r}
ci_up_bound <- p_diffs_mean + p_diffs_mean_sd
ci_low_bound <- p_diffs_mean - p_diffs_mean_sd

# CI string formatter
fmt <- function (ci_bound) format(round(ci_bound, 4), trim = TRUE)

sprintf(
  "A substantial majority* of sampled differences lie between %s and %s.",
  fmt(ci_low_bound), fmt(ci_up_bound)
)
```
<sup>*68%, assuming a normal distribution</sup>

##### Visualize

```{r}
reps_df <- tibble(reps$t)
# Calculate the number of bins with Sturges' rule
nbin <- ceiling(log2(1000) + 1)
ci_up_bound_95 <- quantile(reps$t, 0.975)
ci_low_bound_95 <- quantile(reps$t, 0.025)

ggplot(reps_df, aes(x = reps$t)) +
  geom_histogram(fill = "lightblue", color = "black", bins = nbin) +
  labs(
    title = "Null Distribution of Conversion Rate Differences",
    x = "Difference in Conversion Rate (Treatment - Control)",
    y = "Number of Samples"
  ) +
  geom_vline(xintercept = obs_diff, color = "#CD001A", linetype = "solid") +
  geom_rect(
    xmin = ci_low_bound_95, xmax = ci_up_bound_95,
    ymin = -Inf, ymax = Inf, fill = "#E6BBAD", alpha = 0.01
  ) +
  annotate(
    "text", x = obs_diff + .004625, y = 235, label = paste(
      "Observed Difference =", fmt(obs_diff)
    ),
    color = "#CD001A"
  ) +
  annotate(
    "text", x = obs_diff + .004625, y = 210, label = paste(
      "95% Confidence Interval:\n", fmt(ci_low_bound_95),
      'to', fmt(ci_up_bound_95)
    ),
    color = "#808080"
  ) +
  theme_minimal()
```

The observed difference (in red) falls well within the range of likely values
for the true, population difference under the null hypothesis.

#### P-value

The p-value signifies the probability of a test result being at least as extreme
as the result in the sample--in the direction of the alternate hypothesis--
assuming the null hypothesis is true.

- A high p-value indicates that the observed result (a) is not extreme and (b)
is likely to occur under the null hypothesis, which we thereby fail to reject.
- But a low p-value suggests that the observed result (a) _is_ extreme and (b)
is unlikely to occur under the null hypothesis, which is evidence to reject it.

> ##### Understanding P-values
> <sup> Imagine flipping a coin 100 times. Normally, you'd expect about 50 heads
and 50 tails—the "business-as-usual" or _null_ hypothesis ($H_0$). But if you
got an unusual outcome, like 64 heads, it might raise suspicions about the
coin's fairness--the null hypothesis is assuming a fair coin. <br> &emsp;&emsp;
P-value analysis can help determine how likely it is, under the null hypothesis,
to get such an extreme result. In this case, the odds of landing 64 heads are,
assuming the coin is fair, approximately 1 in 1250 (0.0008)! Now, a p-value of
0.0008 is well below the usual significance level ($\alpha$ = 0.05), which casts
doubt on the notion that the coin is fair--and may constitute grounds to reject
the null hypothesis.

To reject the null hypothesis, our observed statistic must have extremely low
odds of occurring under the null hypothesis: namely, the p-value needs to be
smaller than our chosen significance level ($\alpha$ = 0.05).

```{r}
# P-value
mean(reps$t > obs_diff)
```

In this case, we obtain a relatively large p-value of 0.887, which suggests
that--under the null hypothesis--we are actually likely to observe a difference
($p_{new} - p_{old}$) upwards of -0.0016, the observed statistic. Therefore, we
fail to reject the null hypothesis.

#### Sensitivity

Let us assess the robustness and reliability of our findings. To what extent are
they affected by input or methodology?

```{r}
# Using another random seed...
set.seed(25)
```

```{r}
# Generate another list of bootstrap samples
reps_2 <- boot(data = tbl_2, statistic = ab_test_bootfun, R = 1000)
```

```{r}
# Collect garbage: free unused memory
invisible(gc())
```

```{r}
# P-value
mean(reps_2$t > obs_diff)
```

Here, we obtain a p-value virtually identical to the last one (0.887).

#### Z-score

Z-score offers another way to compare the two conversion rates, representing
the distance between them in terms of standard error ($\text{SE}$). It applies
to situations where the sample is large, the data is distributed normally, and
the population parameters are known--or assumed, among other criteria.
The z-score can be derived from the two sample z-test.

##### Two Sample z-test

This test assess the difference between the means of two samples, accounting for
their variances and sample sizes:

$$
z_{score} = \frac{(p'_{\vphantom{l}new} - p'_{old}) - (p_{new} - p_{old})}
{SE \,(p'_{\vphantom{l}new} - p'_{old})}
$$

where,

$$
SE \,(p'_{\vphantom{l}new} - p'_{old}) =
\sqrt{\frac{s_{\vphantom{l}new}^2}{n_{new}} + \frac{s_{old}^2}{n_{old}}}
$$

<sup>Note: In this context, $p'_{\vphantom{l}new}$ and $p'_{old}$ represent
sample-specific, estimated rates ($\overline{x}$); $p_{new}$ and $p_{old}$
represent the true, population conversion rates ($\mu$), the difference of which
is assumed to be $0$.</sup>

```{r}
z.test(
  x = tbl_2_treat$converted, y = tbl_2_ctrl$converted, alternative = "greater",
  mu = 0, sigma.x = sd(tbl_2_ctrl$converted),
  sigma.y = sd(tbl_2_treat$converted), conf.level = 0.95
)
```

```{r}
# The significance level
alpha <- 0.05

# Calculate the critical z-value for a right-tailed test (95% CI)
qnorm(1 - alpha)
```

For a right-tailed test, given a confidence interval of 95%, the critical value
($z_{\alpha}$) is 1.645. Thus, we would consider z-scores greater than 1.645
standard errors (SE) above 0 as statistically  significant, providing evidence
to reject the null hypothesis.

However, our z-score is -1.311, between 1 and 2 standard errors _below_ 0--
or between the 68% and 95% CI, in line with previous estimates (see [Bootstrap]
above). Similarly, the estimated p-value of 0.905 correlates with earlier
findings (0.888 and 0.887).

Given the z-score of -1.311, as well as a p-value of 0.905, we once again fail
to reject the null hypothesis.

### Bonus: Regression

We can also analyze the test data by conducting a regression analysis. Instead
of a standard linear regression ($\beta_0 + \beta_1X$), we will opt for logistic
regression. While all types of regression aim to uncover statistical
relationships between one or more dependent variables ($Y$) and one or more
independent variables ($X$), logistic regression is tailored toward predicting
binary outcomes. Thanks to the _logistic function_,

$$
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
$$

a logistic regression will never output values less than 0 or greater than 1.
The output will always taper off as it approaches the extremes of its range
(cf linear regressions).*

<sup>*Any time you fit a straight line to yes's and no's--`1`s and `0`s--you can
bet that, eventually, for some values of X, the odds will be less than 0 or
greater than 1. Absurd!</sup>

#### Logistic Regression

By reworking the formula above,

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}
$$

and taking the logarithm of both sides, we arrive at the logistic regression
model.

$$
\text{log}\biggl(\frac{p(X)}{1 - p(X)}\biggl) \,= \beta_0 + \beta_1X 
$$

In this model, increasing $X$ by one unit, from `0` to `1`--that is, from
`control` to `treatment` ($p_{new} - p_{old}$)--changes the log odds by
$\beta_1X$. Since log odds are centered around 0, a negative number indicates
worse odds, and vice versa. So, building on our hypothesis above:

$$
H_0: p_{new} - p_{old} = 0 = \beta_1X\\ H_1: p_{new} - p_{old} \ne 0 \ne \beta_1X
$$
In other words, the null hypothesis is that conversion rate does not depend on landing page, new or old.

```{r}
model <- glm(formula = converted ~ group, data = tbl_2, family = "binomial")
```

```{r}
summary(model)
```

The `treatment` group's odds of a user converting are estimated to be worse
($B_1X$ = -0.015 &#177; 0.011). But because the margin of error approaches 0,
we lack confidence in its odds being significantly different.

```{r}
# The significance level
alpha <- 0.05

# Calculate the critical z-value for a right-tailed test (95% CI)
qnorm(1 - alpha/2)
```

Moving on, the z-score of -1.311 wasn't enough to pass a one-tailed test (see
[Two Sample z-test] above). For a two-tailed test like this one, the critical
value ($z_{\alpha}$) is even larger at &#177; 1.96.

Finally, the p-value is 0.19, which actually corresponds to the estimates
observed earlier. For instance, the two-sample z-test found a p-value of 0.905,
estimating that 90.5% of test statistics would be greater than or equal to the
one we observed. Now, in this two-tailed test, we find the estimated probability
of seeing a value at least as extreme _in either direction_ (0.19 = (1 - 0.905)
* 2). At any rate, 0.19 is greater than our significance level of 0.05.

For all of these reasons, we fail to reject the null hypothesis.