---
title: "Analyze A/B Test Results"
output: html_notebook
---

```{r}
library(boot)
library(BSDA)
library(dplyr)
library(ggplot2)
library(readr)
```

## Wrangle

## Gather and Assess

```{r}
tbl <- read_csv("ab_data.csv")
```

```{r}
head(tbl)
```

```{r}
records <- nrow(tbl)
records
```

```{r}
# Number of records with unique users
records_unique_usr <- length(unique(tbl$user_id))
records_unique_usr
```

```{r}
records_non_unique_usr <- records - records_unique_usr
records_non_unique_usr
```

There are 3894 records with non-unique user IDs.

```{r}
# Preview records with non-unique user IDs
tbl %>%
  filter(duplicated(user_id) | duplicated(user_id, fromLast = TRUE)) %>%
  arrange(user_id) %>%
  slice_head(n = 10)
```

The redundant user IDs have mismatched records! For instance, user `630052` is
part of the `treatment` group. As such, this user should have been served the
`new_page`. Yet user `630052` is recorded as having received both. Similarly,
user `630320` is recorded as being part of the `control` group _and_ the
`treatment` group. So in which group is the user? In both instances, user
`630320` is said to have landed on the `old_page`, which should have been served
to the `control` group.

```{r}
treat_old <- tbl %>%
  filter(group == "treatment" & landing_page != "new_page")

nrow(treat_old)
```

```{r}
ctrl_new <- tbl %>%
  filter(group == "control" & landing_page != "old_page")

nrow(ctrl_new)
```

```{r}
mismatch <- bind_rows(treat_old, ctrl_new)
mismatch_cnt <- nrow(mismatch)

mismatch_cnt
```

```{r}
records_non_unique_usr - mismatch_cnt
```

All but _one_ of the non-unique user IDs are due to redundant, mismatched
record pairs.

```{r}
# Return non-unique user IDs _not_ due to redundant, mismatched record pairs.
tbl %>%
  anti_join(mismatch) %>%
  filter(duplicated(user_id) | duplicated(user_id, fromLast = TRUE))
```

```{r}
# Test whether there are any missing values in the data frame as a whole.
colSums(is.na(tbl)) > 0
```

## Clean

### Issue 1: There are 3893 redundant user IDs having mismatched records.

The `control` group should be served the `old_page`, and the `new_group` should
be served the `new_page`.

#### Define: Filter for the properly matched results, storing them in a new data frame.

#### Code

```{r}
tbl_2 <- tbl %>%
  filter(
    (group == "control" & landing_page == "old_page") |
    (group == "treatment" & landing_page == "new_page")
  )
```

#### Test

```{r}
tbl_2 %>%
  filter(
    (group == "control" & landing_page != "old_page") |
    (group == "treatment" & landing_page != "new_page")
  )
```

### Issue 2: There is one other non-unique user ID.

Note: The two records have different timestamps. Since the variables of
interest are `group`, `landing_page`, and `conversion`, one of the records can
be dropped.

#### Define: Drop one of the records.

#### Code

```{r}
tbl_2 <- tbl_2 %>%
  distinct(user_id, .keep_all = TRUE)
```

#### Test

```{r}
tbl_2 %>%
  filter(duplicated(user_id) | duplicated(user_id, fromLast = TRUE))
```

## Analyze

### Describe: Probability

```{r}
# Overall probability of a user converting
p_pop <- mean(tbl_2$converted)
p_pop
```

```{r}
# Control group's probability of a user converting
tbl_2_ctrl <- tbl_2 %>%
  filter(group == "control")
ctrl_cnvt_prob <- mean(tbl_2_ctrl$converted)

ctrl_cnvt_prob
```

```{r}
# Treatment group's probability of a user converting
tbl_2_treat <- tbl_2 %>%
  filter(group == "treatment")
treat_cnvt_prob <- mean(tbl_2_treat$converted)

treat_cnvt_prob
```

```{r}
# Actual difference between each group's probability of a user converting
obs_diff <- treat_cnvt_prob - ctrl_cnvt_prob
obs_diff
```

<sup>Note: `control` is subtracted from `treatment` to show how exposure to the
new landing page affects the outcome of interest `conversion`: thus, a negative
number indicates a lower probability of conversion, and a positive number
indicates a higher probability.</sup>

In our data sample, we observe that the probability of conversion for the
treatment group is lower--albeit by only 0.0016. But how can we ensure that this
accurately represents the user population as a whole--and isn't just due to
variability and randomness?

### A/B Test Results

As a starting point, let us assume that the `old_page` is better ($H_0$). To
accept the `new_page` as better ($H_1$), we will require conclusive evidence
with, say, a 95% chance of being right--ie a 5% chance of being wrong ($α$ =
0.05).

$$
H_0: p_{new} - p_{old} \leq 0\\ H_1: p_{new} - p_{old} > 0
$$

Under the null hypothesis $H_0$, our aforementioned starting point, we assume
that $p_{new}$ and $p_{old}$ are equal for the purposes of the experiment.
Further, we assume that both are equal to $p_{pop}$, the overall probability of
a user converting across our population. Basically, we assume the
landing page served makes no difference.

#### Bootstrap

We can use the bootstrap technique to measure the level of uncertainty. By
repeatedly resampling observations from the data set with replacement, we
can obtain, say, 1000 estimates for our statistic ie the difference in
conversion rates between treatment and control ($\overline{x}_1 -
\overline{x}_2$). Then, we can approximate the true difference, that is, the
difference for the overall population by taking the mean of the differences,
i.e.,
$$
\overline{B} = \frac{1}{1000} \sum_{r'=1}^{1000}
\,(\overline{x}_1^{\,*r'} -\overline{x}_2^{\,*r'})
$$
along with its standard deviation:
$$
\mathrm{SD}_{\overline{B}} \,(\overline{x}_1 -\overline{x}_2) =
\sqrt{\frac{1}{1000 - 1} \sum_{r'=1}^{1000} \,(\overline{x}_1^{\,*r'} -
\overline{x}_2^{\,*r'} - \overline{B})^2}
$$

##### Resample

```{r}
ab_test_bootfun <- function(data, indices) {
  sample_ctrl <- data[indices, ]
  sample_treat <- data[-indices, ]
  # Probability of a user converting for the bootstrap samples
  sample_ctrl_cnvt_prob <- mean(sample_ctrl$converted)
  sample_treat_cnvt_prob <- mean(sample_treat$converted)
  # Calculate the difference between the conversion rates
  p_diffs <- sample_treat_cnvt_prob - sample_ctrl_cnvt_prob
  return(p_diffs)
}
```

```{r}
set.seed(42)
```

```{r}
reps <- boot(data = tbl_2, statistic = ab_test_bootfun, R = 1000)
```

```{r}
# Collect garbage: free unused memory
invisible(gc())
```

```{r}
# Approximate true difference
p_diffs_mean <- mean(reps$t)
p_diffs_mean
```

```{r}
# And standard deviation
p_diffs_mean_sd <- sd(reps$t)
p_diffs_mean_sd
```

```{r}
ci_up_bound <- p_diffs_mean + p_diffs_mean_sd
ci_low_bound <- p_diffs_mean - p_diffs_mean_sd

# CI string formatter
fmt <- function (ci_bound) format(round(ci_bound, 4), trim = TRUE)

sprintf(
  "A substantial majority* of sampled differences lie between %s and %s.",
  fmt(ci_low_bound), fmt(ci_up_bound)
)
```
<sup>*68%, assuming a normal distribution</sup>

##### Visualize

```{r}
reps_df <- tibble(reps$t)
# Calculate the number of bins with Sturges' rule
nbin <- ceiling(log2(1000) + 1)
ci_up_bound_95 <- quantile(reps$t, 0.975)
ci_low_bound_95 <- quantile(reps$t, 0.025)

ggplot(reps_df, aes(x = reps$t)) +
  geom_histogram(fill = "lightblue", color = "black", bins = nbin) +
  labs(
    title = "Null Distribution of Conversion Rate Differences",
    x = "Difference in Conversion Rate (Treatment - Control)",
    y = "Number of Samples"
  ) +
  geom_vline(xintercept = obs_diff, color = "#CD001A", linetype = "solid") +
  geom_rect(
    xmin = ci_low_bound_95, xmax = ci_up_bound_95,
    ymin = -Inf, ymax = Inf, fill = "#E6BBAD", alpha = 0.01
  ) +
  annotate(
    "text", x = obs_diff + .004625, y = 235, label = paste(
      "Observed Difference =", fmt(obs_diff)
    ),
    color = "#CD001A"
  ) +
  annotate(
    "text", x = obs_diff + .004625, y = 210, label = paste(
      "95% Confidence Interval:\n", fmt(ci_low_bound_95),
      'to', fmt(ci_up_bound_95)
    ),
    color = "#808080"
  ) +
  theme_minimal()
```

The observed difference (in red) falls well within the range of likely values
for the true, population difference under the null hypothesis.

#### P-value

The p-value signifies the probability of a test result being at least as extreme
as the result in the sample--in the direction of the alternate hypothesis--
assuming the null hypothesis is true.

- A high p-value indicates that the observed result (a) is not extreme and (b)
is likely to occur under the null hypothesis, which we thereby fail to reject.
- But a low p-value suggests that the observed result (a) _is_ extreme and (b)
is unlikely to occur under the null hypothesis, which is evidence to reject it.

> ##### Understanding P-values
> <sup> Imagine flipping a coin 100 times. Normally, you'd expect about 50 heads
and 50 tails—the "business-as-usual" or _null_ hypothesis ($H_0$). But if you
got an unusual outcome, like 64 heads, it might raise suspicions about the
coin's fairness--the null hypothesis is assuming a fair coin. <br> &emsp;&emsp;
P-value analysis can help determine how likely it is, under the null hypothesis,
to get such an extreme result. In this case, the odds of landing 64 heads are,
assuming the coin is fair, approximately 1 in 1250 (0.0008)! Now, a p-value of
0.0008 is well below the usual significance level ($\alpha$ = 0.05), which casts
doubt on the notion that the coin is fair--and may constitute grounds to reject
the null hypothesis.

To reject the null hypothesis, our observed statistic must have extremely low
odds of occurring under the null hypothesis: namely, the p-value needs to be
smaller than our chosen significance level ($\alpha$ = 0.05).

```{r}
# P-value
mean(reps$t > obs_diff)
```

In this case, we obtain a relatively large p-value of 0.887, which suggests
that--under the null hypothesis--we are actually likely to observe a difference
($p_{new} - p_{old}$) upwards of -0.0016, the observed statistic. Therefore, we
fail to reject the null hypothesis.